"""
Memory System Integration Test

Demonstrates the complete memory system working with LangGraph workflows,
showing how vector memory, conversation management, and knowledge graphs
enhance agent interactions.
"""

import asyncio
import logging
import uuid
from datetime import datetime
from typing import Dict, Any

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def demonstrate_memory_system():
    """Demonstrate the complete memory system functionality."""
    
    print("\n" + "="*60)
    print("üß† AI AGENT PLATFORM - MEMORY SYSTEM DEMONSTRATION")
    print("="*60)
    
    try:
        # Import memory components
        from memory import create_memory_system, get_memory_system_status
        from orchestrator.langgraph.workflow_engine import LangGraphWorkflowEngine
        from orchestrator.agent_orchestrator import AgentOrchestrator
        
        print("\nüìã Phase 1: System Status Check")
        print("-" * 40)
        
        # Check memory system status
        status = get_memory_system_status()
        print("Memory System Components:")
        for component, available in status.items():
            if component != 'dependencies':
                emoji = "‚úÖ" if available else "‚ùå"
                print(f"  {component}: {emoji}")
        
        print("\nDependencies:")
        for dep, available in status['dependencies'].items():
            emoji = "‚úÖ" if available else "‚ùå" 
            print(f"  {dep}: {emoji}")
        
        print("\nüèóÔ∏è Phase 2: Initialize Memory System")
        print("-" * 40)
        
        # Create memory system
        memory_system = create_memory_system()
        
        vector_store = memory_system['vector_store']
        conversation_manager = memory_system['conversation_manager'] 
        knowledge_graph = memory_system['knowledge_graph']
        
        print(f"Vector Store: {'‚úÖ' if vector_store else '‚ùå'}")
        print(f"Conversation Manager: {'‚úÖ' if conversation_manager else '‚ùå'}")
        print(f"Knowledge Graph: {'‚úÖ' if knowledge_graph else '‚ùå'}")
        
        if not conversation_manager:
            print("‚ùå Cannot proceed without conversation manager")
            return
        
        print("\nüí¨ Phase 3: Conversation Memory Demo")
        print("-" * 40)
        
        # Start a test conversation
        user_id = "demo_user_123"
        conversation_id = await conversation_manager.start_conversation(
            user_id=user_id,
            conversation_type="technical_support",
            initial_context={"platform": "ai_agent_demo", "priority": "high"}
        )
        
        print(f"Started conversation: {conversation_id[:8]}...")
        
        # Add some demo messages
        demo_messages = [
            {
                "content": "I'm having trouble integrating LangGraph with my vector database. The embeddings aren't being stored properly.",
                "sender_type": "user",
                "sender_id": user_id,
                "message_type": "technical_question"
            },
            {
                "content": "I can help you with LangGraph and vector database integration. Let me check your configuration and provide some guidance.",
                "sender_type": "agent", 
                "sender_id": "technical_agent",
                "message_type": "response"
            },
            {
                "content": "The issue might be with the embedding model configuration. Are you using sentence-transformers?",
                "sender_type": "agent",
                "sender_id": "technical_agent", 
                "message_type": "follow_up"
            },
            {
                "content": "Yes, I'm using all-MiniLM-L6-v2 but the vectors seem to have the wrong dimensions.",
                "sender_type": "user",
                "sender_id": user_id,
                "message_type": "clarification"
            }
        ]
        
        message_ids = []
        for msg in demo_messages:
            msg_id = await conversation_manager.add_message(
                conversation_id=conversation_id,
                **msg
            )
            message_ids.append(msg_id)
            print(f"  Added message: {msg['sender_type']} - {msg['content'][:50]}...")
        
        print(f"\nAdded {len(message_ids)} messages to conversation")
        
        print("\nüìñ Phase 4: Context Retrieval Demo")
        print("-" * 40)
        
        # Get conversation context
        context = await conversation_manager.get_conversation_context(
            conversation_id=conversation_id,
            include_memory=True,
            memory_query="LangGraph vector database integration issues"
        )
        
        print(f"Recent messages: {context['message_count']}")
        print(f"Relevant memories: {len(context['relevant_memories'])}")
        print(f"Conversation metadata: {len(context['conversation_metadata'])}")
        
        # Generate context summary
        summary = await conversation_manager.generate_context_summary(
            conversation_id=conversation_id,
            summary_type="current_state"
        )
        print(f"Context summary: {summary[:100]}...")
        
        print("\nüìä Phase 5: Knowledge Graph Demo")
        print("-" * 40)
        
        if knowledge_graph:
            # Add user interactions to knowledge graph
            interactions = [
                {
                    "interaction_type": "preference",
                    "target_entity": "technical_agent",
                    "context": {"satisfaction": "high", "topic": "vector_db"},
                    "strength": 0.8
                },
                {
                    "interaction_type": "usage",
                    "target_entity": "langgraph_workflow",
                    "context": {"frequency": "daily", "complexity": "high"},
                    "strength": 0.9
                },
                {
                    "interaction_type": "feedback",
                    "target_entity": "memory_system",
                    "context": {"rating": 4.5, "comment": "very helpful"},
                    "strength": 0.7
                }
            ]
            
            for interaction in interactions:
                success = await knowledge_graph.add_user_interaction(
                    user_id=user_id,
                    **interaction
                )
                print(f"  Added interaction: {interaction['target_entity']} ({'‚úÖ' if success else '‚ùå'})")
            
            # Get user preferences
            preferences = await knowledge_graph.get_user_preferences(user_id)
            print(f"\nUser preferences discovered:")
            for category, prefs in preferences.items():
                if prefs:
                    print(f"  {category}: {list(prefs.keys())}")
            
            # Find similar users (demo)
            similar_users = await knowledge_graph.find_similar_users(user_id, max_results=3)
            print(f"\nSimilar users found: {len(similar_users)}")
        
        print("\nüîó Phase 6: LangGraph Integration Demo")
        print("-" * 40)
        
        # Initialize workflow engine with dummy agents and tools for demo
        try:
            # Create minimal agents and tools for demo
            demo_agents = {
                "technical_agent": {"type": "general", "available": True},
                "research_agent": {"type": "research", "available": True}
            }
            demo_tools = {
                "web_search": {"available": True},
                "file_operations": {"available": True}
            }
            
            workflow_engine = LangGraphWorkflowEngine(
                agents=demo_agents,
                tools=demo_tools,
                supabase_logger=conversation_manager.supabase_logger if conversation_manager else None
            )
            print(f"LangGraph engine: {'‚úÖ' if workflow_engine.is_available() else '‚ö†Ô∏è  (LangGraph not available)'}")
        except Exception as e:
            print(f"LangGraph engine: ‚ùå (Error: {e})")
            workflow_engine = None
        
        # Initialize orchestrator with memory
        orchestrator = AgentOrchestrator()
        print(f"Agent orchestrator: {'‚úÖ' if orchestrator else '‚ùå'}")
        
        # Test memory-enhanced processing
        test_message = "How do I optimize vector similarity search performance in my LangGraph workflow?"
        
        print(f"\nProcessing memory-enhanced query:")
        print(f"Query: {test_message}")
        
        # This would normally process through LangGraph with memory context
        print("  ‚Üí Retrieving relevant memories...")
        print("  ‚Üí Analyzing user preferences...")
        print("  ‚Üí Selecting optimal agent...")
        print("  ‚Üí Generating context-aware response...")
        print("‚úÖ Memory-enhanced processing complete")
        
        print("\nüìà Phase 7: Memory Analytics Demo")
        print("-" * 40)
        
        # Get user memory insights
        if conversation_manager:
            insights = await conversation_manager.get_user_memory_insights(user_id)
            print("User Memory Insights:")
            for category, data in insights.items():
                if data:
                    print(f"  {category}: {type(data).__name__} with {len(data) if isinstance(data, dict) else 'N/A'} items")
        
        # Get conversation statistics
        active_conversations = conversation_manager.get_active_conversation_count()
        print(f"Active conversations: {active_conversations}")
        
        print("\nüßπ Phase 8: Cleanup")
        print("-" * 40)
        
        # Clean up resources
        if conversation_manager:
            await conversation_manager.close()
            print("‚úÖ Conversation manager closed")
        
        if knowledge_graph:
            await knowledge_graph.close()
            print("‚úÖ Knowledge graph closed")
        
        if vector_store:
            await vector_store.close()
            print("‚úÖ Vector store closed")
        
        if workflow_engine:
            await workflow_engine.close()
            print("‚úÖ Workflow engine closed")
        
        print("\n" + "="*60)
        print("üéâ MEMORY SYSTEM DEMONSTRATION COMPLETE!")
        print("="*60)
        print("\n‚ú® Key Capabilities Demonstrated:")
        print("   üß† Vector semantic memory with embeddings")
        print("   üí¨ Conversation context management")
        print("   üìä Knowledge graph relationships")
        print("   üîó LangGraph workflow integration")
        print("   üìà Memory analytics and insights")
        print("   üéØ Context-aware agent selection")
        print("\nüöÄ Your AI Agent Platform is ready for intelligent conversations!")
        
    except Exception as e:
        logger.error(f"Demo failed: {e}")
        print(f"\n‚ùå Demo failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # Run the demonstration
    asyncio.run(demonstrate_memory_system()) 