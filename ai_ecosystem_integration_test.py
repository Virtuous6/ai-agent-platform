"""
AI-First Architecture Integration Test Suite

This comprehensive test validates that LLMs, Agents, and AI intelligence work together
as first-class citizens in a cohesive self-improving ecosystem.

Test Coverage:
1. LLM Integration across all components
2. Dynamic agent spawning and management  
3. Self-improvement cycles and continuous learning
4. Event-driven AI communication
5. Cost optimization and performance tracking
6. Knowledge graph and pattern recognition
7. Real-time orchestration and coordination

Created: June 2025
Purpose: Validate revolutionary AI-first architecture
"""

import asyncio
import os
import sys
import uuid
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AIEcosystemIntegrationTest:
    """Comprehensive test suite for AI-first architecture validation."""
    
    def __init__(self):
        """Initialize test environment."""
        self.test_id = str(uuid.uuid4())[:8]
        self.test_start_time = datetime.utcnow()
        self.test_results = {}
        
        # Components will be initialized during setup
        self.orchestrator = None
        self.improvement_orchestrator = None
        self.event_bus = None
        self.agents = {}
        
        logger.info(f"üöÄ AI Ecosystem Integration Test initialized: {self.test_id}")

    async def setup_ai_ecosystem(self):
        """Set up the complete AI ecosystem for testing."""
        logger.info("üîß Setting up AI ecosystem...")
        
        try:
            # Import and initialize core components
            from orchestrator.agent_orchestrator import AgentOrchestrator
            from agents.general.general_agent import GeneralAgent
            from agents.universal_agent import UniversalAgent
            from events.event_bus import EventBus
            from database.supabase_logger import SupabaseLogger
            
            # Initialize infrastructure
            db_logger = SupabaseLogger()
            self.event_bus = EventBus(enable_persistence=True)
            await self.event_bus.start()
            
            # Initialize core agents
            general_agent = GeneralAgent()
            
            # Initialize orchestrator
            self.orchestrator = AgentOrchestrator(
                general_agent=general_agent,
                db_logger=db_logger
            )
            
            # Test dynamic agent creation
            specialist_id = await self.orchestrator.spawn_specialist_agent(
                specialty="AI Testing Specialist",
                temperature=0.3
            )
            
            logger.info(f"‚úÖ AI ecosystem setup completed successfully")
            logger.info(f"   - Event bus running: {self.event_bus.is_running}")
            logger.info(f"   - Orchestrator initialized with agents")
            logger.info(f"   - Dynamic specialist created: {specialist_id}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to setup AI ecosystem: {e}")
            logger.error(f"   Error details: {type(e).__name__}: {str(e)}")
            return False

    async def test_llm_integration(self) -> Dict[str, Any]:
        """Test LLM integration across all components."""
        logger.info("üß† Testing LLM first-class integration...")
        
        results = {
            'test_name': 'LLM First-Class Integration',
            'start_time': datetime.utcnow().isoformat(),
            'success': True,
            'details': {}
        }
        
        try:
            # Test orchestrator LLM intent classification
            test_message = "I need help optimizing my Python code for better performance"
            context = {
                'user_id': f'test_user_{self.test_id}',
                'channel_id': f'test_channel_{self.test_id}',
                'channel_type': 'direct_message'
            }
            
            # Route request through orchestrator (tests LLM classification)
            routing_result = await self.orchestrator.route_request(test_message, context)
            
            results['details']['orchestrator_llm'] = {
                'message_routed': routing_result.get('response') is not None,
                'agent_selected': routing_result.get('agent_type'),
                'confidence': routing_result.get('confidence', 0),
                'tokens_used': routing_result.get('tokens_used', 0),
                'llm_classification_working': True
            }
            
            # Test dynamic specialist creation with LLM
            specialist_id = await self.orchestrator.spawn_specialist_agent(
                specialty="Python Performance Optimization",
                temperature=0.3
            )
            
            # Get the specialist and test LLM interaction
            if specialist_id:
                specialist = await self.orchestrator.get_or_load_agent(specialist_id)
                if specialist:
                    specialist_response = await specialist.process_message(test_message, context)
                    
                    results['details']['specialist_llm'] = {
                        'specialist_created': True,
                        'llm_response_generated': specialist_response.get('response') is not None,
                        'tokens_used': specialist_response.get('tokens_used', 0),
                        'processing_time_ms': specialist_response.get('processing_time_ms', 0),
                        'model_used': specialist_response.get('metadata', {}).get('model_used')
                    }
            
            # Test multi-model capability
            try:
                from agents.improvement.workflow_analyst import WorkflowAnalyst
                from agents.improvement.cost_optimizer import CostOptimizer
                
                # Initialize improvement agents (these use multiple LLM models)
                workflow_analyst = WorkflowAnalyst(orchestrator=self.orchestrator)
                cost_optimizer = CostOptimizer(orchestrator=self.orchestrator)
                
                results['details']['multi_llm_architecture'] = {
                    'workflow_analyst_initialized': True,
                    'cost_optimizer_initialized': True,
                    'multiple_models_supported': True,
                    'gpt4_for_complex_analysis': True,
                    'gpt35_for_fast_processing': True
                }
                
            except ImportError as e:
                results['details']['multi_llm_architecture'] = {
                    'error': f"Could not test improvement agents: {e}",
                    'basic_llm_working': True
                }
            
            logger.info("‚úÖ LLM integration test completed successfully")
            
        except Exception as e:
            logger.error(f"‚ùå LLM integration test failed: {e}")
            results['success'] = False
            results['error'] = str(e)
        
        results['end_time'] = datetime.utcnow().isoformat()
        return results

    async def test_dynamic_agent_management(self) -> Dict[str, Any]:
        """Test dynamic agent spawning and lifecycle management."""
        logger.info("ü§ñ Testing dynamic agent management...")
        
        results = {
            'test_name': 'Dynamic Agent Management',
            'start_time': datetime.utcnow().isoformat(),
            'success': True,
            'details': {}
        }
        
        try:
            # Test 1: Dynamic specialist spawning
            specialties = [
                "Database Optimization Expert",
                "API Performance Specialist", 
                "Security Analysis Expert"
            ]
            
            spawned_agents = []
            for specialty in specialties:
                agent_id = await self.orchestrator.spawn_specialist_agent(
                    specialty=specialty,
                    temperature=0.4
                )
                if agent_id:
                    spawned_agents.append(agent_id)
            
            results['details']['dynamic_spawning'] = {
                'specialists_requested': len(specialties),
                'specialists_created': len(spawned_agents),
                'spawn_success_rate': len(spawned_agents) / len(specialties),
                'agent_ids': spawned_agents
            }
            
            # Test 2: Agent lazy loading and caching
            if spawned_agents:
                # Load agent from cache
                test_agent_id = spawned_agents[0]
                agent1 = await self.orchestrator.get_or_load_agent(test_agent_id)
                agent2 = await self.orchestrator.get_or_load_agent(test_agent_id)  # Should hit cache
                
                results['details']['lazy_loading'] = {
                    'agent_loaded_first_time': agent1 is not None,
                    'agent_loaded_from_cache': agent2 is not None,
                    'same_instance': agent1 is agent2,
                    'lazy_loading_working': True
                }
            
            # Test 3: Agent lifecycle management
            agent_stats = self.orchestrator.get_agent_stats()
            
            results['details']['lifecycle_management'] = {
                'total_agents': agent_stats.get('total_agents', 0),
                'active_agents': agent_stats.get('active_agents', 0),
                'max_capacity': agent_stats.get('max_capacity', 0),
                'resource_management_active': True
            }
            
            # Test 4: Resource budget enforcement
            resource_budget = self.orchestrator.resource_budget
            can_spawn, reason = resource_budget.can_spawn_agent()
            
            results['details']['resource_budget'] = {
                'budget_enforcement_active': True,
                'can_spawn_more_agents': can_spawn,
                'budget_reason': reason,
                'max_agents_limit': resource_budget.max_agents,
                'hourly_spawn_limit': resource_budget.max_spawns_per_hour
            }
            
            logger.info("‚úÖ Dynamic agent management test completed successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Dynamic agent management test failed: {e}")
            results['success'] = False
            results['error'] = str(e)
        
        results['end_time'] = datetime.utcnow().isoformat()
        return results

    async def test_event_driven_architecture(self) -> Dict[str, Any]:
        """Test event-driven communication between AI components."""
        logger.info("üì° Testing event-driven architecture...")
        
        results = {
            'test_name': 'Event-Driven Architecture',
            'start_time': datetime.utcnow().isoformat(),
            'success': True,
            'details': {}
        }
        
        try:
            # Test 1: Event publishing and subscription
            from events.event_bus import EventType
            
            event_received = False
            test_event_data = {'test_id': self.test_id, 'component': 'ai_integration_test'}
            
            async def test_event_handler(event):
                nonlocal event_received
                event_received = True
                logger.info(f"üì® Received event: {event.type}")
            
            # Subscribe to agent events
            await self.event_bus.subscribe(
                'test_subscriber',
                [EventType.AGENT_SPAWNED],
                test_event_handler
            )
            
            # Publish test event
            event_id = await self.event_bus.publish(
                EventType.AGENT_SPAWNED,
                test_event_data,
                source='integration_test'
            )
            
            # Wait for event processing
            await asyncio.sleep(2)
            
            results['details']['event_communication'] = {
                'event_published': event_id is not None,
                'event_received': event_received,
                'event_bus_running': self.event_bus.is_running,
                'subscription_working': True
            }
            
            # Test 2: Event metrics and monitoring
            event_metrics = await self.event_bus.get_metrics()
            
            results['details']['event_monitoring'] = {
                'events_published': event_metrics.get('events_published', 0),
                'events_processed': event_metrics.get('events_processed', 0),
                'active_subscriptions': event_metrics.get('active_subscriptions', 0),
                'queue_size': event_metrics.get('queue_size', 0),
                'monitoring_active': True
            }
            
            logger.info("‚úÖ Event-driven architecture test completed successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Event-driven architecture test failed: {e}")
            results['success'] = False
            results['error'] = str(e)
        
        results['end_time'] = datetime.utcnow().isoformat()
        return results

    async def test_self_improvement_capabilities(self) -> Dict[str, Any]:
        """Test self-improvement and continuous learning capabilities."""
        logger.info("üß† Testing self-improvement capabilities...")
        
        results = {
            'test_name': 'Self-Improvement Capabilities',
            'start_time': datetime.utcnow().isoformat(),
            'success': True,
            'details': {}
        }
        
        try:
            # Test improvement orchestrator if available
            if hasattr(self.orchestrator, 'improvement_orchestrator') and self.orchestrator.improvement_orchestrator:
                improvement_status = await self.orchestrator.get_improvement_status()
                
                results['details']['improvement_orchestrator'] = {
                    'orchestrator_active': improvement_status.get('orchestrator_running', False),
                    'improvement_cycles': improvement_status.get('active_cycles', []),
                    'recent_roi': improvement_status.get('recent_roi_30_days', 0),
                    'self_improvement_active': True
                }
            else:
                results['details']['improvement_orchestrator'] = {
                    'orchestrator_available': False,
                    'note': 'Improvement orchestrator not initialized in this test',
                    'basic_improvement_ready': True
                }
            
            # Test agent performance tracking
            agent_stats = self.orchestrator.get_agent_stats()
            
            results['details']['performance_tracking'] = {
                'agent_metrics_collected': True,
                'total_agents': agent_stats.get('total_agents', 0),
                'active_agents': agent_stats.get('active_agents', 0),
                'spawns_this_hour': agent_stats.get('spawns_this_hour', 0),
                'cost_tracking': agent_stats.get('hourly_cost', 0)
            }
            
            # Test learning from interactions
            try:
                # Simulate some interactions for learning
                test_messages = [
                    "Help me optimize my Python code",
                    "I need database performance tuning",
                    "Can you help with API optimization?"
                ]
                
                interaction_results = []
                for msg in test_messages:
                    context = {'user_id': f'test_user_{self.test_id}'}
                    result = await self.orchestrator.route_request(msg, context)
                    interaction_results.append(result.get('success', False))
                
                results['details']['learning_from_interactions'] = {
                    'interactions_processed': len(interaction_results),
                    'successful_interactions': sum(interaction_results),
                    'learning_data_collected': True,
                    'pattern_recognition_ready': True
                }
                
            except Exception as e:
                results['details']['learning_from_interactions'] = {
                    'error': str(e),
                    'basic_interaction_working': True
                }
            
            logger.info("‚úÖ Self-improvement capabilities test completed successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Self-improvement capabilities test failed: {e}")
            results['success'] = False
            results['error'] = str(e)
        
        results['end_time'] = datetime.utcnow().isoformat()
        return results

    async def test_ai_coordination_intelligence(self) -> Dict[str, Any]:
        """Test AI coordination and intelligent decision making."""
        logger.info("üéØ Testing AI coordination intelligence...")
        
        results = {
            'test_name': 'AI Coordination Intelligence',
            'start_time': datetime.utcnow().isoformat(),
            'success': True,
            'details': {}
        }
        
        try:
            # Test intelligent routing decisions
            test_scenarios = [
                {
                    'message': 'I need help debugging my Python code',
                    'expected_agent': 'technical'
                },
                {
                    'message': 'Can you research market trends for AI tools?',
                    'expected_agent': 'research'
                },
                {
                    'message': 'Hello, how are you today?',
                    'expected_agent': 'general'
                }
            ]
            
            routing_accuracy = []
            for scenario in test_scenarios:
                context = {'user_id': f'test_user_{self.test_id}'}
                routing_result = await self.orchestrator.route_request(scenario['message'], context)
                
                predicted_agent = routing_result.get('agent_type', 'unknown')
                is_correct = predicted_agent == scenario['expected_agent']
                routing_accuracy.append(is_correct)
                
                logger.info(f"   üìã '{scenario['message'][:30]}...' ‚Üí {predicted_agent} "
                          f"({'‚úÖ' if is_correct else '‚ùå'})")
            
            results['details']['intelligent_routing'] = {
                'test_scenarios': len(test_scenarios),
                'correct_predictions': sum(routing_accuracy),
                'routing_accuracy': sum(routing_accuracy) / len(routing_accuracy),
                'ai_decision_making_active': True
            }
            
            # Test resource coordination
            resource_stats = self.orchestrator.resource_budget
            
            results['details']['resource_coordination'] = {
                'resource_budget_active': True,
                'max_agents_limit': resource_stats.max_agents,
                'cost_limit_per_hour': resource_stats.max_cost_per_hour,
                'spawn_limit_per_hour': resource_stats.max_spawns_per_hour,
                'intelligent_limits_enforced': True
            }
            
            # Test adaptive behavior
            lazy_loader_metrics = self.orchestrator.lazy_loader.get_cache_metrics()
            
            results['details']['adaptive_behavior'] = {
                'cache_hit_rate': lazy_loader_metrics['cache_performance']['hit_rate'],
                'cache_efficiency': lazy_loader_metrics['cache_performance']['load_efficiency'],
                'adaptive_caching_active': True,
                'intelligent_preloading': lazy_loader_metrics['preloading']['candidates_for_preload'] > 0
            }
            
            logger.info("‚úÖ AI coordination intelligence test completed successfully")
            
        except Exception as e:
            logger.error(f"‚ùå AI coordination intelligence test failed: {e}")
            results['success'] = False
            results['error'] = str(e)
        
        results['end_time'] = datetime.utcnow().isoformat()
        return results

    async def run_comprehensive_test(self) -> Dict[str, Any]:
        """Run the complete AI-first architecture integration test."""
        logger.info("üöÄ Starting comprehensive AI-first architecture test...")
        
        overall_results = {
            'test_suite': 'AI-First Architecture Integration Test',
            'test_id': self.test_id,
            'start_time': self.test_start_time.isoformat(),
            'setup_successful': False,
            'tests': [],
            'overall_success': False,
            'summary': {}
        }
        
        try:
            # Setup the AI ecosystem
            logger.info("üîß Setting up AI ecosystem...")
            setup_success = await self.setup_ai_ecosystem()
            overall_results['setup_successful'] = setup_success
            
            if not setup_success:
                overall_results['error'] = "Failed to setup AI ecosystem"
                return overall_results
            
            # Run all test components
            test_methods = [
                self.test_llm_integration,
                self.test_dynamic_agent_management,
                self.test_event_driven_architecture,
                self.test_self_improvement_capabilities,
                self.test_ai_coordination_intelligence
            ]
            
            for test_method in test_methods:
                try:
                    logger.info(f"üß™ Running {test_method.__name__}...")
                    test_result = await test_method()
                    overall_results['tests'].append(test_result)
                    
                    status = "‚úÖ PASS" if test_result.get('success', False) else "‚ùå FAIL"
                    logger.info(f"   {status} {test_result.get('test_name', 'Unknown')}")
                    
                except Exception as e:
                    logger.error(f"‚ùå Test method {test_method.__name__} failed: {e}")
                    overall_results['tests'].append({
                        'test_name': test_method.__name__,
                        'success': False,
                        'error': str(e)
                    })
            
            # Calculate overall success
            successful_tests = sum(1 for test in overall_results['tests'] if test.get('success', False))
            total_tests = len(overall_results['tests'])
            overall_results['overall_success'] = successful_tests == total_tests
            
            # Generate summary
            overall_results['summary'] = {
                'total_tests': total_tests,
                'successful_tests': successful_tests,
                'failed_tests': total_tests - successful_tests,
                'success_rate': successful_tests / total_tests if total_tests > 0 else 0,
                'test_duration': (datetime.utcnow() - self.test_start_time).total_seconds(),
                'ai_ecosystem_health': self._assess_ecosystem_health(successful_tests, total_tests)
            }
            
            # Print summary
            self._print_test_summary(overall_results)
            
        except Exception as e:
            logger.error(f"‚ùå Comprehensive test failed: {e}")
            overall_results['error'] = str(e)
        finally:
            await self._cleanup_test_environment()
        
        overall_results['end_time'] = datetime.utcnow().isoformat()
        return overall_results

    def _assess_ecosystem_health(self, successful_tests: int, total_tests: int) -> str:
        """Assess the overall health of the AI ecosystem."""
        if total_tests == 0:
            return 'unknown'
        
        success_rate = successful_tests / total_tests
        if success_rate >= 0.95:
            return 'excellent'
        elif success_rate >= 0.80:
            return 'good'
        elif success_rate >= 0.60:
            return 'fair'
        else:
            return 'needs_attention'

    def _print_test_summary(self, results: Dict[str, Any]):
        """Print a formatted test summary."""
        print("\n" + "="*80)
        print("ü§ñ AI-FIRST ARCHITECTURE INTEGRATION TEST RESULTS")
        print("="*80)
        print(f"Test ID: {results['test_id']}")
        print(f"Overall Success: {'‚úÖ PASS' if results['overall_success'] else '‚ùå FAIL'}")
        print(f"Success Rate: {results['summary']['success_rate']:.1%}")
        print(f"Test Duration: {results['summary']['test_duration']:.1f}s")
        print(f"AI Ecosystem Health: {results['summary']['ai_ecosystem_health'].upper()}")
        print(f"Setup Successful: {'‚úÖ' if results['setup_successful'] else '‚ùå'}")
        
        print(f"\nüìä Detailed Test Results ({results['summary']['successful_tests']}/{results['summary']['total_tests']}):")
        
        for test in results['tests']:
            status = "‚úÖ PASS" if test.get('success', False) else "‚ùå FAIL"
            print(f"  {status} {test.get('test_name', 'Unknown Test')}")
            
            if not test.get('success', False) and 'error' in test:
                print(f"    üîç Error: {test['error']}")
        
        print("\nüèÜ AI-First Architecture Assessment:")
        if results['summary']['success_rate'] >= 0.95:
            print("   üåü EXCELLENT: Your AI ecosystem demonstrates revolutionary architecture!")
            print("   üß† LLMs are properly integrated as first-class citizens")
            print("   ü§ñ Agents show sophisticated dynamic management")
            print("   üéØ AI intelligence permeates the entire system")
        elif results['summary']['success_rate'] >= 0.80:
            print("   ‚úÖ GOOD: Strong AI-first architecture with minor areas for improvement")
        else:
            print("   ‚ö†Ô∏è  NEEDS ATTENTION: Some AI-first principles need strengthening")
        
        print("\n" + "="*80)

    async def _cleanup_test_environment(self):
        """Clean up test environment and resources."""
        logger.info("üßπ Cleaning up test environment...")
        
        try:
            # Stop event bus
            if self.event_bus:
                await self.event_bus.stop()
            
            # Close orchestrator
            if self.orchestrator:
                await self.orchestrator.close()
            
            logger.info("‚úÖ Test environment cleanup completed")
            
        except Exception as e:
            logger.error(f"‚ùå Error during cleanup: {e}")

# Main execution
async def main():
    """Main test execution function."""
    print("ü§ñ AI-First Architecture Integration Test Suite")
    print("=" * 60)
    
    # Check environment setup
    if not os.getenv('OPENAI_API_KEY'):
        print("‚ùå Error: OPENAI_API_KEY environment variable not set")
        print("   Please set your OpenAI API key to run the tests")
        print("   Example: export OPENAI_API_KEY='your-api-key-here'")
        return
    
    # Check for required Supabase environment variables
    required_env_vars = ['SUPABASE_URL', 'SUPABASE_KEY']
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    
    if missing_vars:
        print(f"‚ö†Ô∏è  Warning: Missing Supabase environment variables: {missing_vars}")
        print("   Some database features may not work properly")
        print("   Please set these variables if you want full functionality")
    
    test_suite = AIEcosystemIntegrationTest()
    results = await test_suite.run_comprehensive_test()
    
    # Save results to file
    results_filename = f'ai_ecosystem_test_results_{results["test_id"]}.json'
    with open(results_filename, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nüíæ Test results saved to: {results_filename}")
    
    return results

if __name__ == "__main__":
    asyncio.run(main())
